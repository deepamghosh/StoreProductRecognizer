{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d2c5cfe810>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data as traindata and test data \n",
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.ImageFolder('C:/Users/Mayank/Downloads/trainimages',transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=4,shuffle=True, num_workers=2)\n",
    "test_set = torchvision.datasets.ImageFolder('C:/Users/Mayank/Downloads/testimagess',transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=4,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different classes for classification\n",
    "classes=('CEREAL','CHOCOLATE','COFFEE','HONEY','JAM','JUICE','NUTS','SPICES','TOMATO_SAUCE','MILK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples = 2000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 100\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 800\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1) # convolutional layer\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # convolutional layer\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2,0) # max pooling layer\n",
    "        #self.drop2d = nn.Dropout2d(0.2)\n",
    "        self.fc1 = nn.Linear(128*8*8, 64) # densely connected layer\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = x.view(-1, 128*8*8)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader takes in a dataset and a sampler for loading (num_workers deals with system level memory) \n",
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test loader have constant batch sizes, so we can define them directly\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer function\n",
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 64\n",
      "epochs= 5\n",
      "learning_rate= 0.001\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayank\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "C:\\Users\\Mayank\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 12% \t train_loss: 3.04 took: 20.29s\n",
      "Epoch 1, 25% \t train_loss: 2.99 took: 17.05s\n",
      "Epoch 1, 37% \t train_loss: 2.97 took: 17.02s\n",
      "Epoch 1, 50% \t train_loss: 2.99 took: 17.13s\n",
      "Epoch 1, 62% \t train_loss: 2.98 took: 17.13s\n",
      "Epoch 1, 75% \t train_loss: 2.93 took: 16.82s\n",
      "Epoch 1, 87% \t train_loss: 2.91 took: 17.11s\n",
      "Epoch 1, 100% \t train_loss: 2.95 took: 12.98s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayank\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 3.28\n",
      "Epoch 2, 12% \t train_loss: 2.78 took: 24.61s\n",
      "Epoch 2, 25% \t train_loss: 2.76 took: 23.27s\n",
      "Epoch 2, 37% \t train_loss: 2.75 took: 18.54s\n",
      "Epoch 2, 50% \t train_loss: 2.73 took: 17.65s\n",
      "Epoch 2, 62% \t train_loss: 2.67 took: 17.51s\n",
      "Epoch 2, 75% \t train_loss: 2.66 took: 17.93s\n",
      "Epoch 2, 87% \t train_loss: 2.60 took: 17.84s\n",
      "Epoch 2, 100% \t train_loss: 2.60 took: 14.23s\n",
      "Validation loss = 4.41\n",
      "Epoch 3, 12% \t train_loss: 2.60 took: 21.39s\n",
      "Epoch 3, 25% \t train_loss: 2.49 took: 17.23s\n",
      "Epoch 3, 37% \t train_loss: 2.60 took: 17.54s\n",
      "Epoch 3, 50% \t train_loss: 2.58 took: 17.59s\n",
      "Epoch 3, 62% \t train_loss: 2.40 took: 16.99s\n",
      "Epoch 3, 75% \t train_loss: 2.54 took: 16.93s\n",
      "Epoch 3, 87% \t train_loss: 2.34 took: 16.87s\n",
      "Epoch 3, 100% \t train_loss: 2.41 took: 13.53s\n",
      "Validation loss = 4.56\n",
      "Epoch 4, 12% \t train_loss: 2.38 took: 21.71s\n",
      "Epoch 4, 25% \t train_loss: 2.44 took: 18.27s\n",
      "Epoch 4, 37% \t train_loss: 2.33 took: 17.53s\n",
      "Epoch 4, 50% \t train_loss: 2.41 took: 17.39s\n",
      "Epoch 4, 62% \t train_loss: 2.37 took: 17.89s\n",
      "Epoch 4, 75% \t train_loss: 2.37 took: 17.67s\n",
      "Epoch 4, 87% \t train_loss: 2.56 took: 18.28s\n",
      "Epoch 4, 100% \t train_loss: 2.43 took: 13.67s\n",
      "Validation loss = 4.28\n",
      "Epoch 5, 12% \t train_loss: 2.38 took: 22.18s\n",
      "Epoch 5, 25% \t train_loss: 2.26 took: 17.45s\n",
      "Epoch 5, 37% \t train_loss: 2.23 took: 17.17s\n",
      "Epoch 5, 50% \t train_loss: 2.30 took: 18.16s\n",
      "Epoch 5, 62% \t train_loss: 2.37 took: 16.88s\n",
      "Epoch 5, 75% \t train_loss: 2.15 took: 16.66s\n",
      "Epoch 5, 87% \t train_loss: 2.29 took: 16.92s\n",
      "Epoch 5, 100% \t train_loss: 2.36 took: 13.12s\n",
      "Validation loss = 4.82\n",
      "Training finished, took 737.26s\n"
     ]
    }
   ],
   "source": [
    "CNN = SimpleCNN()\n",
    "trainNet(CNN, batch_size=64, n_epochs=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  JAM CEREAL  MILK HONEY\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputss=CNN(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  JUICE  NUTS CEREAL CHOCOLATE\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputss, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 25 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = CNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CEREAL : 45 %\n",
      "Accuracy of CHOCOLATE : 32 %\n",
      "Accuracy of COFFEE : 20 %\n",
      "Accuracy of HONEY :  9 %\n",
      "Accuracy of   JAM : 51 %\n",
      "Accuracy of JUICE : 67 %\n",
      "Accuracy of  NUTS : 22 %\n",
      "Accuracy of SPICES :  5 %\n",
      "Accuracy of TOMATO_SAUCE :  9 %\n",
      "Accuracy of PASTA :  0 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs =CNN(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
